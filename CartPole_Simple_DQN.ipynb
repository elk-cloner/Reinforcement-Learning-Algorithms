{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elk-cloner/RL/blob/master/CartPole_Simple_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEQK0GLD58BM",
        "colab_type": "code",
        "outputId": "bfc18b4c-a59c-4905-84c3-40d5c8fa1593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "# using https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t guide to\n",
        "# show openai gym env in jupyter\n",
        "!apt-get update\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "!pip install pyglet==1.4.0 # gym requirements\n",
        "!apt-get install cmake\n",
        "!pip install --upgrade setuptools\n",
        "!pip install ez_setup\n",
        "!pip install tensorflow gym gym-retro\n",
        "!pip install git+https://github.com/MaxStrange/retrowrapper.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upload your ROMs and run below command\n",
        "!python -m retro.import ./"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5hzhSVr5FpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import gym\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import base64\n",
        "import random\n",
        "import retro\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as mp\n",
        "import retrowrapper\n",
        "from itertools import count\n",
        "from collections import deque\n",
        "from IPython.display import HTML\n",
        "from gym.wrappers import Monitor\n",
        "from gym import logger as gymlogger\n",
        "gymlogger.set_level(40) #error only\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_func = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "class Experience:\n",
        "    def __init__(self, state, action, reward, next_state, done):\n",
        "        self.state = state\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "        self.next_state = next_state\n",
        "        self.done = done\n",
        "memory_buffer = deque(maxlen=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resize_frame(img):\n",
        "    return tf.image.resize(img, size=(244, 244))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dqn_model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer(input_shape=(244, 244, 3)),\n",
        "        tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4, padding=\"same\", use_bias=True, activation=\"relu\"),\n",
        "        tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2, padding=\"same\", use_bias=True, activation=\"relu\"),\n",
        "        tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\", use_bias=True, activation=\"relu\"),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(units=4096, activation=\"relu\", use_bias=True),\n",
        "        tf.keras.layers.Dense(units=2**12 + 1)\n",
        "    ]\n",
        ")\n",
        "dqn_target_model = tf.keras.models.clone_model(dqn_model)\n",
        "print(dqn_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xx = np.ones(shape=[1, 244, 244, 3])\n",
        "dqn_model(xx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "game = \"SonicTheHedgehog-Genesis\"\n",
        "env1 = retrowrapper.RetroWrapper(game)\n",
        "env2 = retrowrapper.RetroWrapper(game)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# action_map = {k: v}\n",
        "import numpy as np\n",
        "action2index = {tuple(map(int, \"{0:012b}\".format(i))): i for i in range(2**12 + 1) }\n",
        "index2action = {v: k for k, v in action2index.items()}\n",
        "action2index[(0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1)]\n",
        "len(action2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decay_step = 0\n",
        "def select_action(state):\n",
        "    global decay_step\n",
        "    # max 1, decrease linearly until 0.1 in first 1m steps and then fix at 0.1\n",
        "    a = -(1 - 0.1) / 1000000.0\n",
        "    b = 1.0\n",
        "    value = max(0.1, a * float(decay_step) + b)\n",
        "    decay_step += 1\n",
        "    if random.random() < value:\n",
        "        action = env1.action_space.sample()\n",
        "        return action\n",
        "    else:\n",
        "        state = tf.reshape(state, shape=(-1, 244, 244, 3))\n",
        "        action_scores = dqn_model.predict(state)\n",
        "        action = index2action[int(tf.math.argmax(action_scores, axis=1).numpy()[0])]\n",
        "        return np.array(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_model():\n",
        "    if len(memory_buffer) < 32:\n",
        "        print(\"\\n not enough data to optimize data\")\n",
        "        return 0\n",
        "    print(\"\\n\")\n",
        "    print(\"get new batch\")\n",
        "    batch = random.sample(memory_buffer, 32)\n",
        "    y = np.zeros(shape=(32,))\n",
        "    selected_action = np.zeros(shape=(32, 2))\n",
        "    batch_states = []\n",
        "    for i, sample in enumerate(batch):\n",
        "        if sample.done:\n",
        "            y[i] = sample.reward\n",
        "        else:\n",
        "            sample.next_state = tf.reshape(sample.next_state, shape=(-1, 244, 244, 3))\n",
        "            y[i] = sample.reward + 0.999 * tf.math.reduce_max(dqn_target_model.predict(sample.next_state)).numpy()\n",
        "        selected_action[i][0] = i\n",
        "        selected_action[i][1] = sample.action\n",
        "        batch_states.append(sample.state)\n",
        "    batch_states = np.array(batch_states)\n",
        "    with tf.GradientTape() as tape:\n",
        "        x = dqn_model(batch_states)\n",
        "        x = tf.gather_nd(x, selected_action)\n",
        "        loss = loss_func(y, x)\n",
        "        print(f\"loss: {loss}\")\n",
        "        gradient = tape.gradient(loss, dqn_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, dqn_model.trainable_variables))\n",
        "    return 1\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def render_model():\n",
        "    current_state = env2.reset()\n",
        "    dqn_target_model.load_weights(\"./pre_train_model/model.ckpt\")\n",
        "    cnt = 0\n",
        "    episod_reward = []\n",
        "    print(\"start rendering middle model loop...\")\n",
        "    while True:\n",
        "        cnt += 1\n",
        "        current_state = np.array(current_state).reshape(1, -1)\n",
        "        action_score = dqn_target_model.predict(current_state)\n",
        "        action = tf.math.argmax(action_score, axis=1).numpy()[0]\n",
        "        action = np.array(index2action[int(action)])\n",
        "        next_state, reward, done, info = envv.step(action)\n",
        "        episod_reward.append(reward)\n",
        "        current_state = next_state\n",
        "        if done or cnt == 100:\n",
        "            break\n",
        "    print(f\"new oracle reward: {sum(episod_reward)/len(episod_reward)}\")\n",
        "    return True\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "update_steps = 0\n",
        "rew = []\n",
        "for train_step in range(1000000):\n",
        "    state = env1.reset()\n",
        "    state = resize_frame(state)\n",
        "    cnt = 0\n",
        "    while True:\n",
        "        cnt += 1\n",
        "        print(f\"cnt: {cnt}\", end=\"-\")\n",
        "        action = select_action(state)\n",
        "        next_state, reward, done, info = env1.step(action)\n",
        "        next_state = resize_frame(next_state)\n",
        "        rew.append(reward)\n",
        "        if reward <= 0.0:\n",
        "            reward = -1.0\n",
        "        memory_buffer.append(Experience(state, action2index[tuple(action.tolist())], reward, next_state, done))\n",
        "        update_steps += update_model()\n",
        "        if update_steps and update_steps % 10 == 0:\n",
        "            dqn_target_model.set_weights(dqn_model.get_weights())\n",
        "            print(\"oracle got updated\")\n",
        "            dqn_target_model.save_weights(\"./pre_train_model/model.ckpt\")\n",
        "            render_model()\n",
        "        if done or cnt == 100:\n",
        "            break\n",
        "        state = next_state\n",
        "    break\n",
        "print(rew)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CartPole_Simple_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}